\pdfminorversion=7%
\documentclass[aspectratio=169,mathserif,notheorems]{beamer}%
%
\xdef\bookbaseDir{../../bookbase}%
\xdef\sharedDir{../../shared}%
\RequirePackage{\bookbaseDir/styles/slides}%
\RequirePackage{\sharedDir/styles/styles}%
%
\gdef\searchSpace{\ensuremath{\mathbb{X}}}%
\gdef\sespel{\ensuremath{x}}%
\gdef\opti#1{\ensuremath{#1^{\star}}}%
%
%% Print an algorithm box around something.
\protected\gdef\algobox#1#2{\resizebox{#1\paperwidth}{!}{%
\bgroup%
\fboxsep=0pt%
\fboxrule=2pt%
\mbox{\fcolorbox{white}{white}{\bgroup%
\fboxsep=2pt%
\fboxrule=2pt%
\mbox{\fcolorbox{hfuu-red!80}{hfuu-orange!60}{%
\mbox{#2}%
}}\egroup%
}}%
\egroup}}%
%
\definecolor{fOneColor}{HTML}{C00000}%
\definecolor{fTwoColor}{HTML}{0070c0}%
\definecolor{fThreeColor}{HTML}{385723}%
\definecolor{fFourColor}{HTML}{7030a0}%
\definecolor{fFiveColor}{HTML}{0000ff}%
\definecolor{fSixColor}{HTML}{00b050}%
\definecolor{fSevenColor}{HTML}{de4e00}%
%
\protected\gdef\fn#1{%
{\bgroup{%
\ifcase#1\relax\or%
\color{fOneColor}\or%
\color{fTwoColor}\or%
\color{fThreeColor}\or%
\color{fFourColor}\or%
\color{fFiveColor}\or%
\color{fSixColor}\else%
\color{fSevenColor}\fi%
\ensuremath{f_{#1}}
}\egroup}%
}%
%
\title{Frequency Fitness Assignment}%
%
\begin{document}%
\startPresentation%
%
\section{Introduction}%
%
\begin{frame}[t]%
\frametitle{Introduction to Optimization}%
\begin{itemize}%
\item Optimization means finding superlatives.%
\item<2-> Find the \alert{fastest} way to get from Hefei to Beijing.%
\item<3-> Find the \alert{shortest} route through $n$~cities.%
\item<4-> Set the pricing for these apples such that we can get the \alert{largest} revenue when selling them.%
\item<5-> Place all these chips on a circuit board so that they occupy the \alert{smallest} area while we can still properly connect and cool them.%
\item<6-> Find the \alert{cheapest} way to transport these goods from Hefei to Wellington.%
\item<7-> Design an airplane wing with the \alert{least} aerodynamic drag.%
\item<8-> Find a strategy to manage the power of the nodes in this sensor network so that full coverage is guaranteed for the \alert{longest} possible duration.%
\item<9-> And so on.%
\end{itemize}%
\locateGraphic{1}{width=0.85\paperwidth}{\sharedDir/graphics/optimization_superlatives/optimization_superlatives}{0.075}{0.2}%
\locateGraphic{2-}{width=0.35\paperwidth}{\sharedDir/graphics/optimization_superlatives/optimization_superlatives}{0.63}{0.01}%
\end{frame}%
%
\begin{frame}[t]%
\frametitle{Views on Optimization}%
\begin{itemize}%
\only<-1>{\item There are two ways to look at optimization.}%
\only<-2>{\item<2-> The economic view.}%
\item<3-> The mathematical view.%
\end{itemize}%
\locateGraphic{2}{width=0.75\paperwidth}{\sharedDir/graphics/optimization_views/optimization_views_1}{0.125}{0.3}%
\locateGraphic{3}{width=0.75\paperwidth}{\sharedDir/graphics/optimization_views/optimization_views_2}{0.125}{0.3}%
\end{frame}%
%
\begin{frame}%
\frametitle{Example: Traveling Salesperson Problem}%
\parbox{0.373\paperwidth}{%
\begin{itemize}%
%
\item In the \glsFull{TSP}\cite{ABCC2006TTSPACS,LLRKS1985TTSPAGTOCO,GP2002TTSPAIV,WCLTTCMY2014BOAAOSFFTTSP}, the goal is to find the shortest round-ttrip tour through a set of $n$~cities.%
%
\item<2-> The search space \searchSpace\ thus is the set of all possible round-trip tours through these $n$~cities, usually specified as permutations of the first $n$~natural numbers.%
%
\item<3-> The objective function~$f:\searchSpace\mapsto\realNumbers$, subject to minimization, is the length of the tour.%
%
\item<4-> The optimal solution~$\opti{\sespel}\in\searchSpace$ is the shortest possible tour.%
%
\end{itemize}%
}%
%
\locateGraphic{-3}{width=0.525\paperwidth}{\sharedDir/graphics/tsp_china/tsp_china}{0.44}{0.15}%
\locateGraphic{4-}{width=0.525\paperwidth}{\sharedDir/graphics/tsp_china/tsp_china_solution}{0.44}{0.15}%
%
\end{frame}%
%
\begin{frame}%
\frametitle{Example: Maximum Satisfiability Problem}%
\parbox{0.42\paperwidth}{%
\begin{itemize}%
%
\item The goal of the \glsFull{MaxSAT}\cite{HS2004SLSFAA,C1971TCOTPP} problem is to find a setting of \textcolor<2>{red}{$n$~variables} that makes a Boolean formula~$F$ become True. %
The variables appear directly or \textcolor<3>{violet}{negated} in \textcolor<4>{blue}{$m$~OR\nobreakdashes-clauses}, whose results flow into \textcolor<5>{green!40!black}{one AND\nobreakdashes-clause}.%
%
\item<6-> \searchSpace\ is the set of all possible bit strings of length~$n$.%
%
\item<7-> The objective function~$f:\searchSpace\mapsto\realNumbers$ is the number of unsatisfied OR\nobreakdashes-clauses.%
%
\item<8-> The optimum~$\opti{\sespel}\in\searchSpace$ has $f(\opti{\sespel})=0$, i.e., all clauses satisfied, i.e., $F(\opti{\sespel})=\textnormal{True}$.%
%
\end{itemize}%
}%
%
\locateGraphic{-1,6-}{width=0.5\paperwidth}{\sharedDir/graphics/sat/sat_plain}{0.47}{0.3}%
\locateGraphic{2}{width=0.5\paperwidth}{\sharedDir/graphics/sat/sat_variables}{0.47}{0.3}%
\locateGraphic{3}{width=0.5\paperwidth}{\sharedDir/graphics/sat/sat_negated}{0.47}{0.3}%
\locateGraphic{4}{width=0.5\paperwidth}{\sharedDir/graphics/sat/sat_clauses}{0.47}{0.3}%
\locateGraphic{5}{width=0.5\paperwidth}{\sharedDir/graphics/sat/sat_and}{0.47}{0.3}%
%
\end{frame}%
%
\begin{frame}[t]%
\frametitle{Example: Bin Packing Problem}%
\begin{itemize}%
\item The goal of the Bin Packing Problem is to pack $n$~objects, each having a specific size, into as few bins~(also of a given size) as possible.%
\item<2-> The \searchSpace\ comprises all possible packing orders of the $n$~objects.%
\item<3-> The objective function~$f$ is the number of bins needed by a given packing order.%
\item<4-> The optimum~$\opti{\sespel}$ is the packing order requiring the fewest bins.%
\end{itemize}%
\locateGraphic{}{width=0.85\paperwidth}{\sharedDir/graphics/bin_packing/bin_packing}{0.075}{0.5}%
\end{frame}%
%
\begin{frame}[t]%
\frametitle{Optimization is Hard}%
\begin{itemize}%
%
\item Finding the globally optimal solution~\opti{\sespel} from the set of all possible solutions~\searchSpace\ is often an \npHard\ problem.%
%
\item<2-> Currently, there is no algorithm that can \alert{guarantee} to find the optimal solution of \alert{every instance} of a given \npHard\ problem in a runtime that is not longer than polynomial in the size of the problem~(i.e., existing algorithms may need exponential runtime in the \alert{worst case}).%
%
\item<3-> In other words, if we want to guarantee to find the best possible solution~\opti{\sespel} for all possible instances of a problem, we often cannot really be much faster than testing all possible candidate solutions~$\sespel\in\searchSpace$ in the \alert{worst case}.%
%
\end{itemize}%
\end{frame}%
%
\section{Metaheuristic Optimization}%
%
\begin{frame}%
\frametitle{Metaheuristic Optimization}%
\parbox{0.42\paperwidth}{%
\begin{itemize}%
%
\item Metaheuristics follow the Trial-and-Error idea of iterative improvement.%
\item<2-> They drop the guarantee to find the optimal solution.%
\item<3-> They try to find good solution within a feasible runtime.%
\item<4-> They start with random solutions.%
\item<5-> And then roughly follow this cycle.%
%
\end{itemize}%
}%
%
\locateGraphic{5}{width=0.5\paperwidth}{\sharedDir/graphics/metaheuristic_cycle/metaheuristic_cycle_01}{0.47}{0.3}%
%
\locateGraphic{6}{width=0.5\paperwidth}{\sharedDir/graphics/metaheuristic_cycle/metaheuristic_cycle_02}{0.47}{0.3}%
%
\locateGraphic{7}{width=0.5\paperwidth}{\sharedDir/graphics/metaheuristic_cycle/metaheuristic_cycle_03}{0.47}{0.3}%
%
\locateGraphic{8}{width=0.5\paperwidth}{\sharedDir/graphics/metaheuristic_cycle/metaheuristic_cycle_04}{0.47}{0.3}%
%
\locateGraphic{9}{width=0.5\paperwidth}{\sharedDir/graphics/metaheuristic_cycle/metaheuristic_cycle_05}{0.47}{0.3}%
%
\locateGraphic{10}{width=0.5\paperwidth}{\sharedDir/graphics/metaheuristic_cycle/metaheuristic_cycle_06}{0.47}{0.3}%
%
\locateGraphic{11}{width=0.5\paperwidth}{\sharedDir/graphics/metaheuristic_cycle/metaheuristic_cycle_07}{0.47}{0.3}%
%
\end{frame}%
%
\begin{frame}[t]%
\frametitle{The $(1+1)$~EA and RLS}%
\begin{itemize}%
\item Local search with $|S_i|=|N_i|=1$ is the simplest realization of the metaheuristic idea.%
\item<2-> \GlsFull{algoRLS} and the \glsFull{algoEAopo} work according the same pattern~(and differ only in their unary search operator~$\mathop{move}$)\cite{DJW2002OTAOTOPOEA,CPD2018TAMPARAOEA}.%
\item<8-> They accept the new solution if it is better or equally good compared to the current solution.%
\end{itemize}%
%
%
\locate{2}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_1}}%
}{0.125}{0.445}%
%
\locate{3}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_2}}%
}{0.125}{0.445}%
%
\locate{4}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_3}}%
}{0.125}{0.445}%
%
\locate{5}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_4}}%
}{0.125}{0.445}%
%
\locate{6}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_5}}%
}{0.125}{0.445}%
%
\locate{7}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_6}}%
}{0.125}{0.445}%
%
\locate{8}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_7}}%
}{0.125}{0.445}%
%
\locate{9}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_8}}%
}{0.125}{0.445}%
%
\locate{10}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea_9}}%
}{0.125}{0.445}%
%
\locate{11-}{%
\algobox{0.75}{\includegraphics[width=0.75\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea}}%
}{0.125}{0.445}%
%
\end{frame}%
%
\begin{frame}%
\frametitle{Simulated Annealing}%
\parbox{0.41\paperwidth}{%
\begin{itemize}%
\item \glsFull{algoSA}\cite{KGV1983OBSA,C1985TATTTSPAESA,DPSW1982MCTICO,P1970AMCMFTASOCTOCOP} is a local search that accepts also worsening moves, but with a probability that decreases over time AND with the difference in solution quality.%
\item<7-> The probability is regulated by temperature schedule with parameter\only<10->{s}~$T_0$\only<10->{ and~$\epsilon$}.%
\item<11-> It also remembers best-so-far solution~$x_B$ and its objective value~$y_B$, because it could get lost.%
\end{itemize}%
}%
%
%
\locate{2}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_01}}%
}{0.44}{0.24}%
%
\locate{3}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_02}}%
}{0.44}{0.24}%
%
\locate{4}{%
\algobox{0.4}{\includegraphics[width=0.4\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_02}}%
}{0.44}{0.07}%
%
\locate{4}{%
\resizebox{0.5\paperwidth}{!}{\bgroup%
\fboxsep=2pt%
\fboxrule=2pt%
\fcolorbox{black}{white}{%
\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/exp}%
}%
\egroup}%%
}{0.46}{0.41}%
%
\locate{5}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_03}}%
}{0.44}{0.24}%
%
\locate{6}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_04}}%
}{0.44}{0.24}%
%
\locate{7}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_05}}%
}{0.44}{0.24}%
%
\locate{8}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_06}}%
}{0.44}{0.24}%
%
\locate{9}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_07}}%
}{0.44}{0.24}%
%
\locate{10}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_08}}%
}{0.44}{0.24}%
%
\locate{11}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_09}}%
}{0.44}{0.24}%
%
\locate{12}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa_10}}%
}{0.44}{0.24}%
%
\locate{13}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sa/sa}}%
}{0.44}{0.24}%
%
\end{frame}%
%
\begin{frame}%
\frametitle{Standard Genetic Algorithm with Roulette Wheel Selection}%
\parbox{0.41\paperwidth}{%
\begin{itemize}%
\item The \glsFull{algoSGA} with Fitness Proportionate Selection~(Roulette Wheel) is for \textcolor{blue}{maximization}\cite{G1989GA,DJ2006ECAUA,W2009GOATAA,BFM1997HOEC,M1996GADSEP,M1998AITGA}.%
\item<2-> It uses a population of size~$ps$ as well as a unary\only<9->{ and binary} operator\only<9->{~(with crossover rate~$cr$)}.%
\end{itemize}%
}%
%
\locate{2}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_01}}%
}{0.44}{0.24}%
%
\locate{3}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_02}}%
}{0.44}{0.24}%
%
\locate{4}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_03}}%
}{0.44}{0.24}%
%
\locate{5}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_04}}%
}{0.44}{0.24}%
%
\locate{6}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_05}}%
}{0.44}{0.24}%
%
\locate{7}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_06}}%
}{0.44}{0.24}%
%
\locate{8}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_07}}%
}{0.44}{0.24}%
%
\locate{9}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_08}}%
}{0.44}{0.24}%
%
\locate{10}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_09}}%
}{0.44}{0.24}%
%
\locate{11}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_10}}%
}{0.44}{0.24}%
%
\locate{12}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_11}}%
}{0.44}{0.24}%
%
\locate{13}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_12}}%
}{0.44}{0.24}%
%
\locate{14}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga_13}}%
}{0.44}{0.24}%
%
\locate{15}{%
\algobox{0.5}{\includegraphics[width=0.5\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/sga/sga}}%
}{0.44}{0.24}%
%
\end{frame}%
%
\begin{frame}%
\frametitle{Acceptance Criteria}%
\begin{itemize}%
\item We have now three example algorithms, \gls{algoEAopo}, \gls{algoSA}, and \gls{algoSGA}.%
%
\item<2-> Assume that they all work on a population of two solutions, where the new solution be~$x_n$ and the old/parent solution be~$x_c$\only<3->{ and we look at the probability~$P$ to accept~$x_n$}.%
%
\item<3-> For the \gls{algoEAopo}, $P=1$ if~$f(x_n)\leq f(x_c)$ and~$0$~otherwise.%
%
\item<4-> In the \gls{algoSA}, $P$~depends on the difference~$\Delta=f(x_c)-f(x_n)$ and is~$1$~if $x_n$ is better than~$x_c$.%
%
\item<5-> In an \gls{algoSGA} for maximization, $P$~is proportional to~$f(x_n)/(f(x_c)+f(x_n))$.\uncover<6->{ If we wanted to do minimization, maybe we could use $1-f(x_n)/(f(x_c)+f(x_n))$ instead.}%
%
\item<7-> \gls{algoEAopo} and \gls{algoSA} always accept~$x_n$ if it is better than~$x_c$, while in the \gls{algoSGA}, a better~$x_n$ at least has a~$P>0.5$.%
%
\item<8-> Let's keep these things in mind.%
\end{itemize}%
\end{frame}%
%
\begin{frame}%
\frametitle{Metaheuristic Optimization}%
\begin{itemize}%
\item What we have learned is that different metaheuristics realize the trial-and-error scheme differently.%
\item<2-> During their selection steps, they all prefer better solutions over worse ones.%
\item<3-> If they would always and only accept the better solutions, they could get trapped in local optima.%
\item<4-> So some of them sometimes accept worse solutions, but the probability to choose a better solution is always higher in average.%
\item<5-> This is the most fundamental concept of metaheuristic optimization:%
\end{itemize}%
%
\uncover<5->{\smallskip%
\begin{center}\textbf{\large{\textcolor{red}{%
If you keep good solutions and modify them, you are likely to get better solutions.%
\uncover<6->{\medskip\\%
If you keep accepting better and better solutions, you will get really good solutions eventually.%
}%
}}}\end{center}%
}%
%
\end{frame}%
%
%
\section{Invariance Properties}%
%
\begin{frame}%
\frametitle{Invariance Properties}%
\begin{itemize}%
\item Research in optimization, \glsFull{ML}, and \glsFull{AI} often use simple problems to test and benchmark algorithms.%
\item<2-> These allow for many experiments in a short time.%
\item<3-> We often know the optimal solutions or at least bounds for the optimal objective values.%
\item<4-> We can understand the results well.%
\item<5-> What we want is that algorithms perform similar to our benchmarking results also on actual, real-world problems.%
\item<6-> \alert{We want invariance properties.}\cite{OAAH2017IGOAAUPVIP,HA2014PDOCSSFTTP,JA2010LLCOTSIMMLEAOMFIRFLPS}%
\end{itemize}%
\end{frame}%
%
%
\begin{frame}%
\frametitle{Invariance Properties: OneMax Example}%
\parbox{0.415\paperwidth}{%
\begin{itemize}%
\item OneMax is the simplest benchmark problem in discrete optimization.%
\item<2-> It is defined over~$\searchSpace=\{0,1\}^n$, i.e., the bit strings of length~$n$.%
\item<3-> Goal:~\inQuotes{Find the bit string of all 1s!}%
\item<4-> \inQuotes{Maximize the number of 1s!}%
\item<5-> $\fn{1}(x)=n-\sum{x}$~(for minimization).%
\item<6-> Every reasonable algorithm can solve this problem.%
\end{itemize}%
}%
%
\locateGraphic{}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1}{0.455}{0.27}%
%
\end{frame}%
%
%
\begin{frame}%
\frametitle{Invariance Properties: OneMax Example -- Translation}%
\parbox{0.415\paperwidth}{%
\begin{itemize}%
\item Now I create a slightly modified variant of this problem.%
\item<2-> $\fn{2}(x)=\fn{1}(x)+10$.%
\item<3-> Expectation: Any reasonable algorithm should perform the same on~$\fn{1}$ and~$\fn{2}$\only<-3>{.}\uncover<4->{, i.e., make the exact same decisions\only<5->{ given the same random seeds}.}%
%
\item<8-> $\fn{1}(x_c)=14$\uncover<9->{, $\fn{1}(x_n)=6$\uncover<10->{, $\fn{2}(x_c)=24$\uncover<11->{, $\fn{2}(x_n)=16$}}}%

\item<12-> \gls{algoEAopo}: $6\leq14$ \textcolor{greenYesColor}{$\mathbf{=}$} $16\leq24$ \greenYes%
%
\item<13-> \gls{algoSA}: $14-6$ \textcolor{greenYesColor}{$\mathbf{=}$} $24-16$ \greenYes%
%
\item<14-> \gls{algoSGA}: $6/(14+6)=0.3$ \textcolor{redNoColor}{$\mathbf{\neq}$} $16/(24+16)=0.4$ \redNo%
\end{itemize}%
}%
%
\locateGraphic{1}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1}{0.455}{0.27}%
\locateGraphic{2-5}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2}{0.455}{0.27}%
\locateGraphic{6}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_xcxn_1}{0.455}{0.27}%
\locateGraphic{7}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_xcxn_2}{0.455}{0.27}%
\locateGraphic{8}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_xcxn_3}{0.455}{0.27}%
\locateGraphic{9}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_xcxn_4}{0.455}{0.27}%
\locateGraphic{10}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_xcxn_5}{0.455}{0.27}%
\locateGraphic{11-}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_xcxn_6}{0.455}{0.27}%
%
\locate{15-}{\algobox{0.6}{%
\parbox{0.6\paperwidth}{%
\begin{itemize}%
\item The \gls{algoEAopo} and \gls{algoSA} are invariant under translations of the objective function value.%
\item<16-> The \gls{algoSGA} is not, i.e., may give us different results for~$\fn{1}$ and~$\fn{2}$.%
\end{itemize}%
}}}{0.2}{0.3}%
%
\end{frame}%
%
%
\begin{frame}%
\frametitle{Invariance Properties: OneMax Example -- Scaling}%
\parbox{0.415\paperwidth}{%
\begin{itemize}%
\item Now I create another slightly modified variant of this problem.%
\item<2-> $\fn{3}(x)=0.8\fn{1}(x)$.%
\item<3-> Expectation: Any reasonable algorithm should perform the same on~$\fn{1}$ and~$\fn{3}$, i.e., make the exact same decisions given the same random seeds.%
%
\item<5-> $\fn{1}(x_c)=14$, $\fn{1}(x_n)=6$\uncover<6->{, $\fn{3}(x_c)=11.2$, $\fn{3}(x_n)=4.8$}%
%
\item<7-> \gls{algoEAopo}: $6\leq14$ \textcolor{greenYesColor}{$\mathbf{=}$} $4.8\leq11.2$ \greenYes%
%
\item<8-> \gls{algoSA}: $14-6$ \textcolor{redNoColor}{$\mathbf{\neq}$} $11.2-4.8$ \redNo%
%
\item<9-> \gls{algoSGA}: $6/(14+6)=0.3$ \textcolor{greenYesColor}{$\mathbf{=}$} $4.8/(11.2+4.8)=0.3$ \greenYes%
\end{itemize}%
}%
%
\locateGraphic{1}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2}{0.455}{0.27}%
\locateGraphic{2-3}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_f3}{0.455}{0.27}%
\locateGraphic{4}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_f3_xcxn_1}{0.455}{0.27}%
\locateGraphic{5}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_f3_xcxn_2}{0.455}{0.27}%
\locateGraphic{6-}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_f3_xcxn_3}{0.455}{0.27}%
%
\locate{10-}{\algobox{0.6}{%
\parbox{0.6\paperwidth}{%
\begin{itemize}%
\item The \gls{algoEAopo} and \gls{algoSGA} are invariant under scaling of the objective function value.%
\item<11-> \gls{algoSA} is not, i.e., may give us different results for~$\fn{1}$ and~$\fn{3}$.%
\end{itemize}%
}}}{0.2}{0.3}%
%
\end{frame}%
%
\begin{frame}%
\frametitle{Invariance Properties: OneMax Example -- Squaring}%
\parbox{0.415\paperwidth}{%
\begin{itemize}%
\item Now I create another slightly modified variant of this problem.%
\item<2-> $\fn{4}(x)=[\fn{1}(x)]^2$.%
\item<3-> Expectation: A nice algorithm should perform the same on~$\fn{1}$ and~$\fn{3}$, i.e., make the exact same decisions given the same random seeds.%
%
\item<4-> $\fn{1}(x_c)=14$, $\fn{1}(x_n)=6$\uncover<5->{, $\fn{4}(x_c)=196$, $\fn{4}(x_n)=36$}%
%
\item<6-> \gls{algoEAopo}: $6\leq14$ \textcolor{greenYesColor}{$\mathbf{=}$} $36\leq196$ \greenYes%
%
\item<7-> \gls{algoSA}: $14-6$ \textcolor{redNoColor}{$\mathbf{\neq}$} $196-36$ \redNo%
%
\item<8-> \gls{algoSGA}: $6/(14+6)=0.3$ \textcolor{redNoColor}{$\mathbf{\neq}$} $36/(196+36)\approx0.156$ \redNo%
\end{itemize}%
}%
%
\locateGraphic{1}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_f3}{0.455}{0.27}%
\locateGraphic{2-3}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_f3_f4}{0.455}{0.27}%
\locateGraphic{4-}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_f3_f4_xcxn}{0.455}{0.27}%
%
\locate{9-}{\algobox{0.6}{%
\parbox{0.6\paperwidth}{%
\begin{itemize}%
\item The \gls{algoEAopo} is invariant under all order-preserving transformations of the objective function value.%
\item<10-> \gls{algoSA} and the \gls{algoSGA} are not, i.e., may give us different results for~$\fn{1}$ and~$\fn{4}$.%
\end{itemize}%
}}}{0.2}{0.3}%
%
\end{frame}%
%
\begin{frame}%
\centering%
\strut\\\strut\vfill\strut\\\strut%
{\large{Now let's enter eerie territory\dots}}%
\strut\\\strut\vfill\strut\\\strut%
\end{frame}%
%
\begin{frame}%
\frametitle{Invariance Properties: OneMax Example -- Trap}%
\parbox{0.415\paperwidth}{%
\begin{itemize}%
\item Now I create another modified variant of this problem:~a trap\cite{DJW2002OTAOTOPOEA,NB2003AAOTBOSEAOTF}.%
\item<2-> \mbox{$\fn{5}(x)=\left\{\begin{array}{rl}0&\textnormal{if~}\fn{1}(x)=50\\1+\fn{1}(x)&\textnormal{otherwise}\end{array}\right.$\hspace{-5em}}%
\item<4-> Expectation: Algorithm performance on~\fn{1} does not carry over to~\fn{5}.
\item<5-> Neither the \gls{algoEAopo}, \gls{algoSA}, nor the \gls{algoSGA} can deal with this well.%
\item<6-> The \gls{algoEAopo} has expected runtime~\bigThetab{n^n} on traps\cite{DJW2002OTAOTOPOEA}.%
\end{itemize}%
}%
\locateGraphic{1}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f2_f3_f4}{0.455}{0.27}%
\locateGraphic{2}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f5}{0.455}{0.27}%
\locateGraphic{3-}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f5_optswap}{0.455}{0.27}%
\end{frame}%
%
\begin{frame}%
\frametitle{Invariance Properties: OneMax Example -- Jump}%
\parbox{0.415\paperwidth}{%
\begin{itemize}%
\item Now I create another modified variant of this problem:~a jump of width~$\omega=10$~bits.%
\item<2-> $\fn{6}(x)$ has a deceptive area of $\omega-1$ bits before the optimum.%
\item<3-> Expectation: Algorithm performance on~\fn{1} does not carry over to~\fn{6}.%
\item<4-> Neither the \gls{algoEAopo}, \gls{algoSA}, nor the \gls{algoSGA} can deal with this well.%
\item<5-> The \gls{algoEAopo} has expected runtime~\bigThetab{n^{\omega} + n\ln{n}} on jumps\cite{DJW2002OTAOTOPOEA}.%
\end{itemize}%
}%
\locateGraphic{}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f6}{0.455}{0.27}%
\end{frame}%
%
\begin{frame}%
\frametitle{Invariance Properties: OneMax Example -- Injection}%
\parbox{0.415\paperwidth}{%
\begin{itemize}%
\item How about I apply an arbitrary injection~$g$ that preserves the location of optimum to~\fn{1} and get~$\fn{7}(x)=g(\fn{1}(x))$?.%
\item<2-> Expectation: Algorithm performance on~\fn{1} is probably unrelated to performance on~\fn{7}.%
\item<3-> Neither the \gls{algoEAopo}, \gls{algoSA}, nor the \gls{algoSGA} can deal with this well.%
\item<4-> Indeed, \alert{no} existing algorithm can deal with this, let alone have some invariance properties with respect to \emph{that}.%
\end{itemize}%
}%
\locateGraphic{}{width=0.51\paperwidth}{\sharedDir/graphics/onemax/onemax_f1_f7}{0.455}{0.27}%
\end{frame}%
%
\begin{frame}%
\uncover<-2>{%
Indeed, \alert{no} existing algorithm can deal with this, let alone have some invariance properties with respect to \emph{that}.%
\uncover<2->{%
\\\strut\\well\dots%
}}%
\locateGraphic{3}{width=0.6\paperwidth}{\sharedDir/graphics/papers/WWLC2021FFAMOAIUBTOTOFV}{0.2}{0.04}%
%
\locate{3}{\parbox{0.9\paperwidth}{%
\tiny{T Weise, Z Wu, X Li, and Y Chen. \citetitle{WWLC2021FFAMOAIUBTOTOFV}. IEEE Transactions on Evolutionary Computation~25(2):307–319. 2021.~\bracketCite{WWLC2021FFAMOAIUBTOTOFV}}%
}}{0.05}{0.91}%
%
\end{frame}%
%
\section{Frequency Fitness Assignment}%
%
\begin{frame}%
\frametitle{FFA: Idea}%
\begin{itemize}%
\item \GlsFull{algoFFA} is a module that can be plugged into different existing algorithms.%
\item<2-> It changes the way the algorithm selects the interesting solutions~$S_{i+1}$ from the sets~$P_i=S_i\cup N_i$.%
\item<3-> It therefore maintains a table~$H$ with the encounter frequency of each objective value in the selection decisions.%
\item<4-> The table~$H$ is initially filled with zeros.%
\item<5-> Before the selection step of the algorithm, $H[f(P_i[j])]$ for all $j\in\intRange{1}{|P_i|}$ is incremented by~1.%
\item<6-> Then, the frequencies~$H[f(P_i[j])]$ replace the objective values $f(P_i[j])$ in the actual selection decisions.%
\end{itemize}%
\end{frame}%
%
\begin{frame}[t]%
\frametitle{FFA: (1+1)~EA und (1+1)~FEA}%
%
\begin{itemize}%
\only<-1>{%
\item Let's plug \gls{algoFFA} into the \gls{algoEAopo} and obtain the \glsFull{algoFEAopo}.%
}%
\only<-2>{%
\item<2-> We start with the \gls{algoEAopo}.%
}\only<-3>{%
\item<3-> We begin by initializing the frequency table~$H$ with all zeros.%
}\only<-4>{%
\item<4-> \emph{Before} the selection decision, we increment the frequency values of the objective values of all current solutions.%
}\only<-5>{%
\item<5-> Now the frequency values replace the objective values in the selection decisions.%
}%
\item<6-> Since we may now lose the best-so-far solution, we need to track it in additional variables.%
\item<8-> {\dots}which are then the return values of the \gls{algoFEAopo}.%
\end{itemize}%
%
\locate{}{%
\algobox{0.45}{\includegraphics[width=0.45\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/opoea/opoea}}%
}{0.033333333}{0.4}%
%
%
\locate{2-}{%
\algobox{0.45}{%
\only<2>{\includegraphics[width=0.45\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/fea/fea_1}}%
\only<3>{\includegraphics[width=0.45\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/fea/fea_2}}%
\only<4>{\includegraphics[width=0.45\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/fea/fea_3}}%
\only<5>{\includegraphics[width=0.45\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/fea/fea_4}}%
\only<6>{\includegraphics[width=0.45\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/fea/fea_5}}%
\only<7>{\includegraphics[width=0.45\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/fea/fea_6}}%
\only<8->{\includegraphics[width=0.45\paperwidth]{\sharedDir/graphics/metaheuristic_algorithms/fea/fea}}%
}%
}{0.516666666}{0.3}%
%
\end{frame}%
%
\input{\sharedDir/advertisement/advertisement.tex}%
%
\endPresentation%
\end{document}%%
\endinput%
%
